{"title":"Introduction to probability and statistics","markdown":{"yaml":{"title":"Introduction to probability and statistics","author":"Esteban Montenegro-Montenegro, PhD","institute":"Psychology and Child Development","title-slide-attributes":{"data-background-image":"stan_state_screen_1.png","data-background-opacity":"0.5"},"format":{"revealjs":{"theme":"simple","slide-number":true,"self-contained":true}},"editor":"visual","bibliography":"references.bib","csl":"apa.csl"},"headingText":"Reality, Nature, Science, and Models","headingAttr":{"id":"","classes":["smaller","smaller"],"keyvalue":[["background-image","slide2.png"],["background-opacity","0.5"]]},"containsRefs":false,"markdown":"\n\n\n-   It might seem trivial to talk about nature and how science related to nature. However, we will study ***nature*** when we study **statistics**.\n\n-   As @westfall2013understanding mentioned in their book: \"Nature is all aspects of past, present, and future existence. Understanding Nature requires common observation---that is, it encompasses those things that we can agree we are observing\" (p.1)\n\n-   As psychologist we study behavior, thoughts, emotions, beliefs, cognition and contextual aspects of all the above. These elements are also part of nature, we mainly study *constructs*, I will talk about constructs frequently.\n\n-   *Statistics* is the language of science. Statistics concerns the analysis of recorded information or data. Data are commonly observed and subject to common agreement and are therefore more likely to reflect our common reality or *Nature*.\n\n## Reality, Nature, Science, and Models II {.smaller .smaller background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n::: columns\n::: {.column width=\"40%\"}\n-   To study and understand *Nature* we must construct a ***model*** for how *Nature* works.\n\n-   A model helps you to understand Nature and also allows you to make predictions about Nature. There is no right or wrong model; they are all wrong! But some are better than others.\n:::\n\n::: {.column width=\"60%\"}\n![](world.png){fig-align=\"right\"}\n:::\n:::\n\n## Statistical Models {.smaller .smaller background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n::: columns\n::: {.column width=\"60%\"}\n-   We will focus on mathematical models integrated by equations and theorems.\n\n-   Models cannot reflect exactly how Nature works, but it is close enough to allow us to make predictions and inferences.\n\n-   Let's create a model for driving distance.Imagine you drive at 100 km/hour, then we can predict your driving time $y$ in hours by creating the following model: $y = x/100$\n\n-   You could plug in any value to replace $x$ and you'll get a prediction:\n\n-   Example:\n\n```{=tex}\n\\begin{align}\ny &= 300 km/100 \\\\\ny & = 3 hours \\\\\n\\end{align}\n```\n:::\n\n::: {.column width=\"40%\"}\n![](modelDistance.png){fig-align=\"right\"}\n:::\n:::\n\n## Model produces data {.smaller .smaller background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n-   Pay attention to our model. The model just created produces data, if we replace $x$ by other values, the model will give us new information:\n\n::: panel-tabset\n## R code\n\n```{r, echo = TRUE}\nkm <- c(200,300,400,500,600,900,1000)\n## Our model\ntime <- km/100\n### Let's plot the information\nlibrary(ggplot2)\n\np <- ggplot(data=data.frame(km,time), aes(x=km ,y=time)) +\n    geom_line() +\n    geom_point()+ \n    theme_classic()\n\n```\n\n## Plot\n\n```{r, fig.height = 4, fig.width = 6}\np\n```\n\n## Data\n\n```{r}\ndata1 <- data.frame(km,time)\ndata1\n\n```\n:::\n\n## Model produces data II {.smaller .smaller background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n-   A Model is like a recipe, it has some steps and rules that will help you to prepare a cake or a meal. The cake is your data.\n\n::: {.callout-important .smaller appearance=\"simple\"}\n## Always remember!\n\nModels produce data, data does not produce models!\n:::\n\n-   You can use data to *estimate* models, but that does not change the fact that your model comes first, before you ever see any data.\n\n-   You can use data to estimate models, but that does not change the fact that your model comes first, before you ever see any data.\n\n## Statistical Processes {.smaller .smaller background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n::: {.callout-important .smaller appearance=\"simple\"}\n## Always remember!\n\nThe order matters, Nature is there before our measurements,the data comes after we establish our way to measure Nature.\n:::\n\n![](nature.png){fig-align=\"right\"}\n\n## Statistical Processes II {.smaller .smaller background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n@westfall2013understanding are doing an important distinction:\n\n-   They present ***DATA*** with upper case to differentiate ***data*** with lower case. Why?\n\n-   When we talk about Nature we are talking about infinite numbers of observations, from these infinite number of possibilities we extract a portion of DATA, this is similar to the concept of ***population***. For instance, your DATA could be all the college students in California.\n\n-   ***data*** (lowercase) means that we already collected a sample from that ***DATA***. For example, if you get information from only college students from valley, you'll have one single instance of what is possible in your population.\n\n-   Your ***data*** is the only way we have to say something about the ***DATA***.\n\n-   Prior collecting data, the DATA is *random*, *unknown*.\n\n## Statistical Processes III {.smaller .smaller background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n*More about models*\n\n-   We will use the term \"probability model\" , we will represent this term using: $p(y)$ which translates into \"probability of $y$\".\n\n-   Let's use the flipping coin example. We know that the probability of flipping a coin and getting heads is 50%, same probability can be observed for tails (50%). Then, we can represent this probability by $p(heads) = 0.5$ or $p(tail) = 0.5$.\n\n-   This is actually a good model! Every time, it produces as many random coin flips as you like. Models produce data means: that YOUR model is a model the explains how your DATA will be produced!\n\n-   Your ***model*** for Nature is that your *DATA* come from a probability model $p(y).$\n\n![](modelP.png){fig-align=\"center\"}\n\n## Statistical Processes IV {.smaller .smaller background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n-   Our model $p(y)$ can be used to predict and explain Nature. A prediction is a guess about unknown events in the past, present or future or about events that might not happen at all.\n\n-   It is more like asking \"What-if\" .For example, what if I had extra \\$3000 to pay my credit balance?\n\n## Deterministic vs. Probabilistic {.smaller .smaller background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n-   In science, you'll find models that are ***deterministic***, that means; our outcome is completely determined by our $x$\n\nLet's see this example: [Free Fall Calculator](https://www.angio.net/personal/climb/speed.html)\n\n-   In physics you'll find several examples of deterministic models, especially from the Newtonian perspective.\n\n-   Normally these models are represented with $f(.)$ instead of $p(.)$, for example the driving distance example can be written as $f(x) = x/100$.\n\n-   This symbol $f(x)$ means mathematical function. The function will give us only one solution in the case of a deterministic mode. We plug in values and we get a solution. ($y = f(x)$).\n\n## Deterministic vs. Probabilistic II {.smaller .smaller background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n-   Probabilistic models models assume *variability* , they are not deterministic, probabilistic models produce data that varies, therefore we'll have distributions.\n\n-   Probabilistic models are more realistic to explain phenomena in psychology.\n\n-   The following expresion represents a probabilistic model:\n\n```{=tex}\n\\begin{equation}\nY \\sim p(y)\n\\end{equation}\n```\n-   The symbol $\\sim$ can be read aloud either as \"produced by\" or \"distributed as.\" In a complete sentence, the mathematical shorthand $Y \\sim p(y)$ states that your $DATA$ $Y$ are produced by a probability model having mathematical form $p(y)$.\n\n## Variability {.smaller .smaller background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n-   Would a deterministic model explain how people feel after a traumatic event?\n\n-   Can we plug in values in a deterministic function to predict your attention span while driving?\n\n-   You must use a *probabilistic* (stochastic) models to study natural variability.\n\n## Parameters {.smaller .smaller background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n-   A ***parameter*** is a numerical characteristic of the data-generating process, one that is usually unknown but often can be estimated using data.\n\n```{=tex}\n\\begin{equation}\nY \\sim \\beta_{0} + \\beta_{1}X\n\\end{equation}\n```\nFor instance, in the model showed above, we have ***two unknown parameters***, this model produces data $Y$. The unknown parameters are represented with greek letters, for instance the letter *beta* in the example above.\n\n::: {.callout-important .smaller appearance=\"simple\"}\n## Mantra\n\nModel produces data.\\\nModel has unknown parameters.\\\nData reduce the uncertainty about the unknown parameters.\\\n:::\n\n## Purely Probabilistic Statistical Models {background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n-   In a probabilistic model the variable $Y$ is produced at random. This statement is represented by $Y \\sim p(y)$.\n\n-   $p(y)$ is called a *probability density function (pdf)*.\n\n-   A pdf assigns a likelihood to your values. I will explain this in the next sessions.\n\n-   ***IMPORTANT:*** A purely probabilistic statistical model states that a variable $Y$ is produced by a pdf having unknown parameters. In symbolic shorthand, the model is given as $Y \\sim p(y|\\theta )$. This greek letter $\\theta$ is called \"theta\".\n\n## Purely Probabilistic Statistical Models II {.smaller .scrollable background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n-   Let's think that Nature is also observable in our class. Our class belongs to the data generating process of grades in stats classes in the world. Let's also assume that the data generating process of grades is normally distributed with a mean of 78 (out of 100) and standard deviation of 1. We can also imagine that we have a single sample of 100 students that have taken stats classes. Let's check the graph...\n\n::: panel-tabset\n## R code\n\n```{r, echo = TRUE, eval = FALSE}\nset.seed(1234)\n\ndataProcess <- rnorm(16000000, \n                     mean = 78, \n                     sd = 1)\ngrades <- rnorm(100,  \n                mean = 78, \n                sd = 1)\n\nplot(density(dataProcess), \n     lwd = 2, \n     col = \"red\",\n     main = \"DATA generating process\", \n     xlab = \"Grade\", \n     ylab = \"p(x)\")\n\nlines(density(grades), \n      col = \"blue\", \n      lwd = 2)\n\nlegend(80, 0.4, \n       legend=c(\"Data Process\", \"My sample\"),\n       col=c(\"red\", \"blue\"), lty=1)\n\n```\n\n## Figure\n\n```{r, echo = FALSE}\nset.seed(1234)\n\ndataProcess <- rnorm(16000000, \n                     mean = 78, \n                     sd = 1)\ngrades <- rnorm(100,  \n                mean = 78, \n                sd = 1)\n\nplot(density(dataProcess), \n     lwd = 2, \n     col = \"red\",\n     main = \"DATA generating process density function\", \n     xlab = \"Grade\", \n     ylab = \"p(x)\")\n\nlines(density(grades), \n      col = \"blue\", \n      lwd = 2)\n\nlegend(80, 0.4, \n       legend=c(\"Data Process\", \"My sample\"),\n       col=c(\"red\", \"blue\"), lty=1)\n\n```\n:::\n\n## Purely Probabilistic Statistical Models III {background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n-   I just used the word \"assume\" , probabilistic models have *assumptions*. In the previous example we assumed:\n\n    -   The data generating process is normally distributed.\n    -   We assumed a value for the mean.\n    -   We assumed a value for the standard deviation.\n\n## Statistical Inference {.smaller background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n-   Normally if we flip a coin we have around 0.50 probability of getting tails, and also around 0.50 of getting heads. But, what would happen if we bend the coin? Are you sure you'll get 0.50 probability? Can we assume the same probability?\n\n| Outcome, $y$ | $p(y)$    |\n|--------------|-----------|\n| Tails        | 1 - $\\pi$ |\n| Heads        | $\\pi$     |\n| Total        | 1.00      |\n\n: Probability distribution for a Bent Coin\n\n-   Now we have uncertainty, we can only assume that there is a probability represented by $\\pi$. That's the only think we know.\n\n## Statistical Inference II {background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n-   We need to collect data to reduce the uncertainty about the unknown parameters.\n\n-   The reduction in uncertainty about model parameters that you achieve when you collect data is called ***statistical inference***\n\n## References {background-image=\"slide2.png\" background-opacity=\"0.5\"}\n","srcMarkdownNoYaml":"\n\n## Reality, Nature, Science, and Models {.smaller .smaller background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n-   It might seem trivial to talk about nature and how science related to nature. However, we will study ***nature*** when we study **statistics**.\n\n-   As @westfall2013understanding mentioned in their book: \"Nature is all aspects of past, present, and future existence. Understanding Nature requires common observation---that is, it encompasses those things that we can agree we are observing\" (p.1)\n\n-   As psychologist we study behavior, thoughts, emotions, beliefs, cognition and contextual aspects of all the above. These elements are also part of nature, we mainly study *constructs*, I will talk about constructs frequently.\n\n-   *Statistics* is the language of science. Statistics concerns the analysis of recorded information or data. Data are commonly observed and subject to common agreement and are therefore more likely to reflect our common reality or *Nature*.\n\n## Reality, Nature, Science, and Models II {.smaller .smaller background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n::: columns\n::: {.column width=\"40%\"}\n-   To study and understand *Nature* we must construct a ***model*** for how *Nature* works.\n\n-   A model helps you to understand Nature and also allows you to make predictions about Nature. There is no right or wrong model; they are all wrong! But some are better than others.\n:::\n\n::: {.column width=\"60%\"}\n![](world.png){fig-align=\"right\"}\n:::\n:::\n\n## Statistical Models {.smaller .smaller background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n::: columns\n::: {.column width=\"60%\"}\n-   We will focus on mathematical models integrated by equations and theorems.\n\n-   Models cannot reflect exactly how Nature works, but it is close enough to allow us to make predictions and inferences.\n\n-   Let's create a model for driving distance.Imagine you drive at 100 km/hour, then we can predict your driving time $y$ in hours by creating the following model: $y = x/100$\n\n-   You could plug in any value to replace $x$ and you'll get a prediction:\n\n-   Example:\n\n```{=tex}\n\\begin{align}\ny &= 300 km/100 \\\\\ny & = 3 hours \\\\\n\\end{align}\n```\n:::\n\n::: {.column width=\"40%\"}\n![](modelDistance.png){fig-align=\"right\"}\n:::\n:::\n\n## Model produces data {.smaller .smaller background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n-   Pay attention to our model. The model just created produces data, if we replace $x$ by other values, the model will give us new information:\n\n::: panel-tabset\n## R code\n\n```{r, echo = TRUE}\nkm <- c(200,300,400,500,600,900,1000)\n## Our model\ntime <- km/100\n### Let's plot the information\nlibrary(ggplot2)\n\np <- ggplot(data=data.frame(km,time), aes(x=km ,y=time)) +\n    geom_line() +\n    geom_point()+ \n    theme_classic()\n\n```\n\n## Plot\n\n```{r, fig.height = 4, fig.width = 6}\np\n```\n\n## Data\n\n```{r}\ndata1 <- data.frame(km,time)\ndata1\n\n```\n:::\n\n## Model produces data II {.smaller .smaller background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n-   A Model is like a recipe, it has some steps and rules that will help you to prepare a cake or a meal. The cake is your data.\n\n::: {.callout-important .smaller appearance=\"simple\"}\n## Always remember!\n\nModels produce data, data does not produce models!\n:::\n\n-   You can use data to *estimate* models, but that does not change the fact that your model comes first, before you ever see any data.\n\n-   You can use data to estimate models, but that does not change the fact that your model comes first, before you ever see any data.\n\n## Statistical Processes {.smaller .smaller background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n::: {.callout-important .smaller appearance=\"simple\"}\n## Always remember!\n\nThe order matters, Nature is there before our measurements,the data comes after we establish our way to measure Nature.\n:::\n\n![](nature.png){fig-align=\"right\"}\n\n## Statistical Processes II {.smaller .smaller background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n@westfall2013understanding are doing an important distinction:\n\n-   They present ***DATA*** with upper case to differentiate ***data*** with lower case. Why?\n\n-   When we talk about Nature we are talking about infinite numbers of observations, from these infinite number of possibilities we extract a portion of DATA, this is similar to the concept of ***population***. For instance, your DATA could be all the college students in California.\n\n-   ***data*** (lowercase) means that we already collected a sample from that ***DATA***. For example, if you get information from only college students from valley, you'll have one single instance of what is possible in your population.\n\n-   Your ***data*** is the only way we have to say something about the ***DATA***.\n\n-   Prior collecting data, the DATA is *random*, *unknown*.\n\n## Statistical Processes III {.smaller .smaller background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n*More about models*\n\n-   We will use the term \"probability model\" , we will represent this term using: $p(y)$ which translates into \"probability of $y$\".\n\n-   Let's use the flipping coin example. We know that the probability of flipping a coin and getting heads is 50%, same probability can be observed for tails (50%). Then, we can represent this probability by $p(heads) = 0.5$ or $p(tail) = 0.5$.\n\n-   This is actually a good model! Every time, it produces as many random coin flips as you like. Models produce data means: that YOUR model is a model the explains how your DATA will be produced!\n\n-   Your ***model*** for Nature is that your *DATA* come from a probability model $p(y).$\n\n![](modelP.png){fig-align=\"center\"}\n\n## Statistical Processes IV {.smaller .smaller background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n-   Our model $p(y)$ can be used to predict and explain Nature. A prediction is a guess about unknown events in the past, present or future or about events that might not happen at all.\n\n-   It is more like asking \"What-if\" .For example, what if I had extra \\$3000 to pay my credit balance?\n\n## Deterministic vs. Probabilistic {.smaller .smaller background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n-   In science, you'll find models that are ***deterministic***, that means; our outcome is completely determined by our $x$\n\nLet's see this example: [Free Fall Calculator](https://www.angio.net/personal/climb/speed.html)\n\n-   In physics you'll find several examples of deterministic models, especially from the Newtonian perspective.\n\n-   Normally these models are represented with $f(.)$ instead of $p(.)$, for example the driving distance example can be written as $f(x) = x/100$.\n\n-   This symbol $f(x)$ means mathematical function. The function will give us only one solution in the case of a deterministic mode. We plug in values and we get a solution. ($y = f(x)$).\n\n## Deterministic vs. Probabilistic II {.smaller .smaller background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n-   Probabilistic models models assume *variability* , they are not deterministic, probabilistic models produce data that varies, therefore we'll have distributions.\n\n-   Probabilistic models are more realistic to explain phenomena in psychology.\n\n-   The following expresion represents a probabilistic model:\n\n```{=tex}\n\\begin{equation}\nY \\sim p(y)\n\\end{equation}\n```\n-   The symbol $\\sim$ can be read aloud either as \"produced by\" or \"distributed as.\" In a complete sentence, the mathematical shorthand $Y \\sim p(y)$ states that your $DATA$ $Y$ are produced by a probability model having mathematical form $p(y)$.\n\n## Variability {.smaller .smaller background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n-   Would a deterministic model explain how people feel after a traumatic event?\n\n-   Can we plug in values in a deterministic function to predict your attention span while driving?\n\n-   You must use a *probabilistic* (stochastic) models to study natural variability.\n\n## Parameters {.smaller .smaller background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n-   A ***parameter*** is a numerical characteristic of the data-generating process, one that is usually unknown but often can be estimated using data.\n\n```{=tex}\n\\begin{equation}\nY \\sim \\beta_{0} + \\beta_{1}X\n\\end{equation}\n```\nFor instance, in the model showed above, we have ***two unknown parameters***, this model produces data $Y$. The unknown parameters are represented with greek letters, for instance the letter *beta* in the example above.\n\n::: {.callout-important .smaller appearance=\"simple\"}\n## Mantra\n\nModel produces data.\\\nModel has unknown parameters.\\\nData reduce the uncertainty about the unknown parameters.\\\n:::\n\n## Purely Probabilistic Statistical Models {background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n-   In a probabilistic model the variable $Y$ is produced at random. This statement is represented by $Y \\sim p(y)$.\n\n-   $p(y)$ is called a *probability density function (pdf)*.\n\n-   A pdf assigns a likelihood to your values. I will explain this in the next sessions.\n\n-   ***IMPORTANT:*** A purely probabilistic statistical model states that a variable $Y$ is produced by a pdf having unknown parameters. In symbolic shorthand, the model is given as $Y \\sim p(y|\\theta )$. This greek letter $\\theta$ is called \"theta\".\n\n## Purely Probabilistic Statistical Models II {.smaller .scrollable background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n-   Let's think that Nature is also observable in our class. Our class belongs to the data generating process of grades in stats classes in the world. Let's also assume that the data generating process of grades is normally distributed with a mean of 78 (out of 100) and standard deviation of 1. We can also imagine that we have a single sample of 100 students that have taken stats classes. Let's check the graph...\n\n::: panel-tabset\n## R code\n\n```{r, echo = TRUE, eval = FALSE}\nset.seed(1234)\n\ndataProcess <- rnorm(16000000, \n                     mean = 78, \n                     sd = 1)\ngrades <- rnorm(100,  \n                mean = 78, \n                sd = 1)\n\nplot(density(dataProcess), \n     lwd = 2, \n     col = \"red\",\n     main = \"DATA generating process\", \n     xlab = \"Grade\", \n     ylab = \"p(x)\")\n\nlines(density(grades), \n      col = \"blue\", \n      lwd = 2)\n\nlegend(80, 0.4, \n       legend=c(\"Data Process\", \"My sample\"),\n       col=c(\"red\", \"blue\"), lty=1)\n\n```\n\n## Figure\n\n```{r, echo = FALSE}\nset.seed(1234)\n\ndataProcess <- rnorm(16000000, \n                     mean = 78, \n                     sd = 1)\ngrades <- rnorm(100,  \n                mean = 78, \n                sd = 1)\n\nplot(density(dataProcess), \n     lwd = 2, \n     col = \"red\",\n     main = \"DATA generating process density function\", \n     xlab = \"Grade\", \n     ylab = \"p(x)\")\n\nlines(density(grades), \n      col = \"blue\", \n      lwd = 2)\n\nlegend(80, 0.4, \n       legend=c(\"Data Process\", \"My sample\"),\n       col=c(\"red\", \"blue\"), lty=1)\n\n```\n:::\n\n## Purely Probabilistic Statistical Models III {background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n-   I just used the word \"assume\" , probabilistic models have *assumptions*. In the previous example we assumed:\n\n    -   The data generating process is normally distributed.\n    -   We assumed a value for the mean.\n    -   We assumed a value for the standard deviation.\n\n## Statistical Inference {.smaller background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n-   Normally if we flip a coin we have around 0.50 probability of getting tails, and also around 0.50 of getting heads. But, what would happen if we bend the coin? Are you sure you'll get 0.50 probability? Can we assume the same probability?\n\n| Outcome, $y$ | $p(y)$    |\n|--------------|-----------|\n| Tails        | 1 - $\\pi$ |\n| Heads        | $\\pi$     |\n| Total        | 1.00      |\n\n: Probability distribution for a Bent Coin\n\n-   Now we have uncertainty, we can only assume that there is a probability represented by $\\pi$. That's the only think we know.\n\n## Statistical Inference II {background-image=\"slide2.png\" background-opacity=\"0.5\"}\n\n-   We need to collect data to reduce the uncertainty about the unknown parameters.\n\n-   The reduction in uncertainty about model parameters that you achieve when you collect data is called ***statistical inference***\n\n## References {background-image=\"slide2.png\" background-opacity=\"0.5\"}\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","self-contained":true,"output-file":"introProbAndStats.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.3.353","auto-stretch":true,"title":"Introduction to probability and statistics","author":"Esteban Montenegro-Montenegro, PhD","institute":"Psychology and Child Development","title-slide-attributes":{"data-background-image":"stan_state_screen_1.png","data-background-opacity":"0.5"},"editor":"visual","bibliography":["references.bib"],"csl":"apa.csl","theme":"simple","slideNumber":true}}},"projectFormats":["html"]}